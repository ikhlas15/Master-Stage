# -*- coding: utf-8 -*-
"""Copy of Last Version/ September

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBFpgZQKzGYB292fw7CwQEiiB5upDCvr

**<div align="center"><h1>Title: Hyperparameter Sizing with Computational Budget and sample sizes</h1></div>**

# **Objective**: <br>
## This notebook aims to explore the size of hyperparameters as a function of sample size and computational time budget. The goal of this internship is to establish a relationship between sample size, computational resources, and the ideal hyperparameter sizes for different models.

## **Step 1: Model Implementation**

### Import Libraries :
"""

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
np.random.seed(5)
import time

"""### Define Functions :"""

def true_function(x):
    return np.sin(2 * np.pi * x)

# the output will be piece wise constant
def partition_estimator_cst(Xtrain, ytrain, Xtest, Jcard):
    ftest = np.zeros(len(Xtest))
    bintest = np.ceil(Xtest * Jcard)
    bintest[bintest == 0] = 1
    bintrain = np.ceil(Xtrain * Jcard)
    bintrain[bintrain == 0] = 1

    for itest in range(len(Xtest)):
        x = Xtest[itest]
        bin_ = bintest[itest]
        ind = np.where(bintrain == bin_)[0]
        if len(ind) == 0:
            ftest[itest] = np.mean(ytrain)
        else:
            ftest[itest] = np.mean(ytrain[ind])

    return ftest
# The output is linear (linear splines) solving it bu least resgression
def partition_estimator_lst(Xtrain, ytrain, Xtest, Jcard):
    ftest = np.zeros(len(Xtest))
    bintest = np.ceil(Xtest * Jcard)
    bintest[bintest == 0] = 1
    bintrain = np.ceil(Xtrain * Jcard)
    bintrain[bintrain == 0] = 1

    for itest in range(len(Xtest)):
        x = Xtest[itest]
        bin_ = bintest[itest]
        ind = np.where(bintrain == bin_)[0]
        if len(ind) == 0:
            ftest[itest] = np.mean(ytrain)
        else:
            # Perform least-squares
            yloc = ytrain[ind]
            Xloc = np.hstack((Xtrain[ind], np.ones((len(ind), 1))))
            theta = np.linalg.lstsq(Xloc, yloc, rcond=None)[0]
            ftest[itest] = np.array([Xtest[itest], 1]).dot(theta)
    return ftest

"""### Generate Data"""

np.random.seed(1)
n = 200
std_noise = 0.2
Xtrain = np.random.rand(n, 1)
Xtrain=np.sort(Xtrain)
def true_function(Xtrain):
    return np.sin(2 * np.pi * Xtrain)
ytrain = true_function(Xtrain) + std_noise * np.random.randn(n, 1)
Xtest = np.arange(0, 1, 0.01).reshape(-1, 1)
ytest = true_function(Xtest)

"""## **1- Partition estimator which called also regressogram**"""

Jcard_values = [10, 50, 100]
plt.figure(figsize=(15, 10))

for i, Jcard in enumerate(Jcard_values):
    # ftest1 plots (let's call it constant estimator as an example)
    ftest1 = partition_estimator_cst(Xtrain, ytrain, Xtest, Jcard)
    plt.subplot(len(Jcard_values), 3, i + 1)
    plt.plot(Xtrain, ytrain, 'xk', markersize=10)
    plt.plot(Xtest, ytest, 'b', linewidth=2, label='target')
    plt.plot(Xtest, ftest1, 'r', linewidth=2, label='cst estimator')
    plt.axis([0, 1, -1.5, 1.5])
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'number of partitions = {Jcard} (cst)', fontweight='normal')
    plt.legend()
    ftest2 = partition_estimator_lst(Xtrain, ytrain, Xtest, Jcard)
    plt.subplot(len(Jcard_values), 3, i + 1 + len(Jcard_values))
    plt.plot(Xtrain, ytrain, 'xk', markersize=10)
    plt.plot(Xtest, ytest, 'b', linewidth=2, label='target')
    plt.plot(Xtest, ftest2, 'g', linewidth=2, label='lst estimator')
    plt.axis([0, 1, -1.5, 1.5])
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'number of partitions = {Jcard} (lst)', fontweight='normal')
    plt.legend()

plt.tight_layout()
plt.show()

"""## **2- K-nearest neighbors**"""

K_values = [1,10,40]
plt.figure(figsize=(18, 5))
for i, k in enumerate(K_values):
    plt.subplot(1, 3, i+1)
    ftest = np.zeros(Xtest.shape[0])
    for itest in range(Xtest.shape[0]):
        x = Xtest[itest]
        b = np.argsort(np.abs(Xtrain - x), axis=0)
        ftest[itest] = np.mean(ytrain[b[:k]])

    # Plot true function, noisy training data, and k-NN estimates
    plt.plot(Xtest, ytest, 'b', linewidth=2, label='True Function')
    plt.plot(Xtrain, ytrain, 'xk', markersize=10, label='Training Data')
    plt.plot(Xtest, ftest, 'r', linewidth=2, label='k-NN')

    # Setting the axis, labels and title
    plt.axis([0, 1, -1.5, 1.5])
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.title(f'k = {k}', fontweight='normal')
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)

# To show the plots
plt.tight_layout()
plt.show()

"""## **3- Nadarya Watson - Kernel regression**"""

def compute_ftest(Xtrain, ytrain, Xtest, h):
  ftest = np.zeros_like(Xtest)
  for itest in range(len(Xtest)):
      x = Xtest[itest]
      temp = np.exp(-((x - Xtrain)**2) / (2 * h * h))
      ftest[itest] = np.dot(temp.T, ytrain) / np.sum(temp)
  return ftest

# Set figure size and plot for different bandwidths
fig, ax = plt.subplots(1, 3, figsize=(18, 5))

h_values = [0.005, 0.05, 0.25]
for i, h in enumerate(h_values):
    ftest = compute_ftest(Xtrain, ytrain, Xtest, h)

    ax[i].plot(Xtest, ytest, 'b', linewidth=2)
    ax[i].plot(Xtest, ftest, 'r', linewidth=2)
    ax[i].plot(Xtrain, ytrain, 'xk', markersize=10)
    ax[i].set_xlim([0, 1])
    ax[i].set_ylim([-1.5,1.5])
    ax[i].set_xlabel('x')
    ax[i].set_ylabel('y')
    ax[i].set_title(f'h = {h:.3f}', fontweight='normal')
    ax[i].legend(['target', 'Nadaraya-W.'])

plt.tight_layout()
plt.savefig("nadaraya_1D.png")
plt.show()

"""## **Step 2: Hyperparameter Sizing** <br>
### The best hyperparamter = $\arg \min_{\theta} \text{MSE}$  <br>
*where*

*    $\theta$ is the hyperparameter.
*   MSE : is the Mean squared error $$\cfrac{1}{N} \sum_{i=1}^{N} (\hat{f}_{\theta}(x_i)) - (f(x_i)+ùõÜ_i))^2$$

## **1- The hyperparameter in Partition estimator:**    $J_{card}$ (the number  of partitions) <br>
## **2- The hyperparameter in K Nearest Neighbors :**    $K$ (the number  of neighbors)<br>
## **3- The hyperparameter in Kernel regression :**    $h$ (the size of the kernel function)

# **Here is the first trail to approximate the function $f(x)=sin(2 \pi x)$** <br>  **by using Polynomial Approximation function**
"""

N = 100
x = np.random.uniform(0, 1, N)
x = np.sort(x)
epsilon = np.random.normal(0, 0.1, N)
def true_function(x):
    return np.sin(2 * np.pi * x)
y = true_function(x) + epsilon

# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)

time_budget = 300
tt1 = time.time()
def polynomial_prediction(theta, x):
    degree = len(theta) - 1
    return np.polyval(theta[::-1], x)

def objective_function(theta, X, y):
    y_pred = polynomial_prediction(theta, X)
    difference = y- y_pred
    return np.linalg.norm(difference)**2 + 0.003* np.linalg.norm(theta)**2, y_pred

def gradient(theta, x, y):
    y_pred = polynomial_prediction(theta, x[1])
    grad = 2 *(y_pred - y) * x  + 2*0.003*theta
    return grad

def stochastic_gradient_descent(X, y, learning_rate=0.05, num_epochs=1000, batch_size=1):
    num_samples = X.shape[0]
    num_features = len(X[0])
    theta = np.zeros(num_features)

    for epoch in range(num_epochs):
        for i in range(0, num_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]
            gradient_sum = np.zeros(num_features)
            for j in range(len(X_batch)):
                gradient_sum += gradient(theta, X_batch[j], y_batch[j])
            gradient_avg = gradient_sum / batch_size
            theta -= learning_rate * gradient_avg

    return theta

best_degree = None
best_objective = float('inf')

for degree in range(1, 15):
    X_train = np.column_stack([x_train ** i for i in range(degree + 1)])
    optimal_theta = stochastic_gradient_descent(X_train, y_train)
    X_val = np.column_stack([x_val ** i for i in range(degree + 1)])
    objective_val, y_val_pred = objective_function(optimal_theta, X_val[:, 1], y_val)
    if degree > 1:
        plt.plot(x_val[np.argsort(x_val)], y_val_pred[np.argsort(x_val)], label="approximation_{}".format(degree))

    if objective_val < best_objective:
        best_objective = objective_val
        best_degree = degree

    tt2 = time.time()
    if tt2 - tt1 >= time_budget:
        print("Time budget exhausted after {:.2f} seconds.".format(tt2 - tt1))
        break

X_train = np.column_stack([x_train ** i for i in range(best_degree + 1)])
X_test = np.column_stack([x_val ** i for i in range(best_degree + 1)])
optimal_theta = stochastic_gradient_descent(X_train, y_train)
objective_test = objective_function(optimal_theta, X_test[:, 1], y_val)
print("Best Degree:", best_degree)
plt.plot(x, y, label="original with noise")
X_train_best = np.column_stack([x_train ** i for i in range(best_degree + 1)])
optimal_theta_best = stochastic_gradient_descent(X_train_best, y_train)
y_pred_best = polynomial_prediction(optimal_theta_best, x)
plt.plot(x, y_pred_best, label="approximation (degree {})".format(best_degree))
plt.legend()

"""# **Step 3: Time Complexity Analysis**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time

sample_sizes = [100,1000,10000,20000,40000,60000,80000,100000,1000000] #for KNN
sample_sizes_p = [100,1000,10000,20000,40000,60000,80000,100000]
def generate_random_data(size):
    return np.random.rand(size), np.random.rand(size), np.random.rand(size)

def partition_estimator_lst(Xtrain, ytrain, Xtest, Jcard):
    ftest = np.zeros(len(Xtest))
    bintest = np.ceil(Xtest * Jcard)
    bintest[bintest == 0] = 1
    bintrain = np.ceil(Xtrain * Jcard)
    bintrain[bintrain == 0] = 1

    for itest in range(len(Xtest)):
        x = Xtest[itest]
        bin_ = bintest[itest]
        ind = np.where(bintrain == bin_)[0]
        if len(ind) == 0:
            ftest[itest] = np.mean(ytrain)
        else:
            # Perform least-squares
            yloc = ytrain[ind]
            Xloc = np.hstack((Xtrain[ind].reshape(-1, 1), np.ones((len(ind), 1))))
            theta = np.linalg.lstsq(Xloc, yloc, rcond=None)[0]
            ftest[itest] = np.array([Xtest[itest], 1]).dot(theta)
    return ftest

def knn_estimator(x_train, y_train, x_validation, k):
    knn_model = KNeighborsRegressor(n_neighbors=k)
    knn_model.fit(x_train.reshape(-1, 1), y_train)
    y_pred = knn_model.predict(x_validation.reshape(-1, 1))
    return y_pred

def kernel_estimator(Xtrain, ytrain, Xtest, h):
  ftest = np.zeros_like(Xtest)
  for itest in range(len(Xtest)):
      x = Xtest[itest]
      temp = np.exp(-((x - Xtrain)**2) / (2 * (h**2)))
      ftest[itest] = np.dot(temp.T, ytrain) / np.sum(temp)
  return ftest

def measure_time(func, *args):
    start_time = time.time()
    func(*args)
    end_time = time.time()
    return end_time - start_time

execution_times_partition = []
for size in sample_sizes_p:
    Xtrain, ytrain, Xtest = generate_random_data(size)
    execution_times_partition.append(measure_time(partition_estimator_lst, Xtrain, ytrain, Xtest, 10))
plt.figure(figsize=(10, 6))
plt.plot(sample_sizes_p, execution_times_partition, '-o', label='Partition Estimator')
plt.xlabel('Sample Size')
plt.ylabel('Execution Time (seconds)')
plt.title('Execution Time vs Sample Size')
plt.legend()
plt.grid(True)
plt.show()

execution_times_knn = []
for size in sample_sizes:
    Xtrain, ytrain, Xtest = generate_random_data(size)
    execution_times_knn.append(measure_time(knn_estimator, Xtrain, ytrain, Xtest, 5))
plt.figure(figsize=(8, 6))
plt.plot(sample_sizes, execution_times_knn, '-o', label='KNN Estimator')
plt.xlabel('Sample Size')
plt.ylabel('Execution Time (seconds)')
plt.title('Execution Time vs Sample Size')
plt.legend()
plt.grid(True)
plt.show()

execution_times_kernel = []
for size in sample_sizes_p:
    Xtrain, ytrain, Xtest = generate_random_data(size)
    execution_times_kernel.append(measure_time(kernel_estimator, Xtrain, ytrain, Xtest, 10))
plt.figure(figsize=(10, 6))
plt.plot(sample_sizes_p, execution_times_kernel, '-o', label='Kernel regression')
plt.xlabel('Sample Size')
plt.ylabel('Execution Time (seconds)')
plt.title('Execution Time vs Sample Size')
plt.legend()
plt.grid(True)
plt.show()

"""# **Step 4: Results and Discussion**

## **1- Define**
*   The sample size.
*   Validation set size.
*   The standard deviation.
*   Scaling list
"""

sample_sizes = [30,100,300,1000,3000,10000,30000]
sample_sizes_ct = [10,1000,10000]
scale_list = [0.1, 0.2, 0.5, 1, 2,3,4]
validation_size = 0.3
sigma = 0.2

"""# **1- Partition estimator(with Least Squares regression)**

"""

# The output is linear (linear splines) solving it bu least resgression
def partition_estimator_lst(Xtrain, ytrain, Xtest, Jcard):
    ftest = np.zeros(len(Xtest))
    bintest = np.ceil(Xtest * Jcard)
    bintest[bintest == 0] = 1
    bintrain = np.ceil(Xtrain * Jcard)
    bintrain[bintrain == 0] = 1

    for itest in range(len(Xtest)):
        x = Xtest[itest]
        bin_ = bintest[itest]
        ind = np.where(bintrain == bin_)[0]
        if len(ind) == 0:
            ftest[itest] = np.mean(ytrain)
        else:
            # Perform least-squares
            yloc = ytrain[ind]
            Xloc = np.hstack((Xtrain[ind], np.ones((len(ind), 1))))
            theta = np.linalg.lstsq(Xloc, yloc, rcond=None)[0]
            ftest[itest] = np.array([Xtest[itest], 1]).dot(theta)
    return ftest

def distance_function_lst(x, y, J, f_star, n_mc):
    x_mc = np.random.rand(n_mc,1)
    f_hat = partition_estimator_lst(x, y, x_mc, J)
    distance = np.mean((np.abs(f_hat - f_star(x_mc)))**2)
    return distance
def MSE(fhat, x_valid, y_valid):
    mse = np.mean((np.abs(fhat - y_valid))**2)
    return mse

fig, ax = plt.subplots(2, 1, figsize=(10,8))
result_table = []
for n in (sample_sizes):
    results = {}
    true_theoretical_J = round((4*n/(16*(sigma**2) +1 ))**(1/3))
    J_list = [round(true_theoretical_J * scale) for scale in scale_list if round(true_theoretical_J * scale) < n*(1-validation_size) and round(true_theoretical_J * scale) != 0]
    J_list = np.unique(J_list)
    print('true_theoretical_J',true_theoretical_J,'J_list',J_list)
    best_mse = float('inf')
    best_p = float('inf')
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
    best_mses = []
    best_hyperparams = []
    for J in J_list:
        time_start = time.time()
        f_validation = partition_estimator_lst(x, y, x_validation, J)
        mse = MSE(f_validation, x_validation, y_validation)
        time_end = time.time()
        T = time_end-time_start
        distance = distance_function_lst(x, y, J, f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_p = J
        c_t = T / (n**2)
        results[J] = distance
        result_table.append({
            "Sample Size": n,
            "Partition Size": J,
            "Best J": best_p,
            "T": T,
            "c_t": c_t,
            "Distance": distance,
            "MSE": mse,
        })
        best_mses.append(mse)
        best_hyperparams.append(J)
    J_values = list(results.keys())
    distances = list(results.values())
    min_J = min(results, key=results.get)
    min_distance = results[min_J]
    print(f"For sample size {n}, J_t : {true_theoretical_J},best no. of partitons =:{best_p},best MSE : {best_mse},min distance : {min_distance} ")
    ax[0].loglog(J_values, distances, marker='o', label=f'Sample Size {n}')
    ax[1].loglog(best_hyperparams, best_mses, marker='o', label=f'Sample Size {n}')

ax[0].set_xlabel('number of partitions')
ax[0].set_ylabel('Best Distance')
ax[0].set_title('Best Distance by J ')
ax[0].legend()
ax[1].set_xlabel('number of partitions ')
ax[1].set_ylabel('MSE on Validation')
ax[1].set_title('MSE on Validation by J')
ax[1].legend()
plt.tight_layout()
plt.show()
result_table = pd.DataFrame(result_table).sort_values('Sample Size')

result_table['c_t']
pscale_factor = np.mean(c_t)
print(pscale_factor)
Time_budget = 1
pNew_scale_list = [Time_budget /(pscale_factor  * (n**2)) for  n in sample_sizes]
print(pNew_scale_list)

"""## **Here we modify the B constant in J_theory value**"""

import tqdm as tqdm
fig, ax = plt.subplots(figsize=(8, 6))
B = 0.05
for idx, n in enumerate(sample_sizes):
    x = np.random.rand(n, 1)
    eps = np.random.normal(0, sigma, n)
    f_star = lambda x_: np.sin(2 * np.pi * x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
    scale_ = int(np.ceil(pNew_scale_list[idx]))
    results = {}
    not_so_theoretical_J = np.ceil((4 * (B ** 2) * n / (16 * (sigma ** 2) + B ** 2)) ** (1 / 3))
    print('no. of partitions with B=',B,'is', not_so_theoretical_J)
    partition_estimator_lst(x, y, x_validation, not_so_theoretical_J)
    J_list_log = np.linspace(0,2* np.log(not_so_theoretical_J),num=scale_)
    J_list = np.unique(np.round(np.exp(J_list_log)))
    if len(J_list) < scale_:
      J_list = np.concatenate((J_list,np.arange(2,min(scale_-len(J_list),n))))
      J_list = np.unique(J_list)
      #print('J_list_merged',J_list)
    if len(J_list)==1:
      J_list=[not_so_theoretical_J]
    if not_so_theoretical_J in J_list:
     print('J_(not theory) exists')
    best_hyperparams_for_n = []
    best_mse = float('inf')
    best_mses = []
    best_hyperparams = []
    for J in J_list:
        time_start = time.time()
        f_validation = partition_estimator_lst(x, y, x_validation, J)
        mse = MSE(f_validation, x_validation, y_validation)
        time_end = time.time()
        T = time_end - time_start
        distance = distance_function_lst(x, y, J, f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_p=J
        results[J] = distance
        best_mses.append(mse)
        best_hyperparams.append(J)
    print('best no. of partitions',best_p,'that corresponds the MSE =',best_mse)
    J_values = list(results.keys())
    distances = list(results.values())
    min_distance, corresponding_J = min(zip(distances, J_values))  # Finds the minimum distance and its corresponding J value
    print(f"For sample size {n}, the lowest distance is: {min_distance}, corresponding to J = {corresponding_J}")
    ax.loglog(J_values, distances, marker='o', label=f'Sample Size {n}')
ax.set_xlabel('J (number of partitions)')
ax.set_ylabel('Best Distance')
ax.set_title('Partition estimator on Smooth function (B lipschitz =0.05)')
ax.legend()
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(8, 6))
B = 100
for idx, n in enumerate(sample_sizes):
  x = np.random.rand(n, 1)
  eps = np.random.normal(0, sigma, n)
  f_star = lambda x_: np.sin(2 * np.pi * x_.ravel())
  y = f_star(x) + eps
  x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
  scale_ = int(np.ceil(pNew_scale_list[idx]))
  results = {}
  not_so_theoretical_J = round((4 * (B ** 2) * n / (16 * (sigma ** 2) + B ** 2)) ** (1 / 3))
  print('J with B lipschtz= 100', not_so_theoretical_J)
  partition_estimator_lst(x, y, x_validation, not_so_theoretical_J)
  J_list_log = np.linspace(0,2* np.log(not_so_theoretical_J),num=scale_)
  J_list = np.unique(np.round(np.exp(J_list_log)))
  if len(J_list) < scale_:
    J_list = np.concatenate((J_list,np.arange(2,min(scale_-len(J_list),n))))
    J_list = np.unique(J_list)
    #print('J_list_merged',J_list)
  if len(J_list)==1:
    J_list=[not_so_theoretical_J]
  if not_so_theoretical_J in J_list:
    print('J_(not theory) exists')
  best_mse = float('inf')
  best_mses = []
  best_hyperparams = []
  for J in J_list:
      time_start = time.time()
      f_validation = partition_estimator_lst(x, y, x_validation, J)
      mse = MSE(f_validation, x_validation, y_validation)
      time_end = time.time()
      T = time_end - time_start
      distance = distance_function_lst(x, y, J, f_star, 1000)
      if mse < best_mse:
          best_mse = mse
          best_p = J
      results[J] = distance
      best_mses.append(mse)
      best_hyperparams.append(J)
  print('best no. of partitions',best_p,'that corresponds the MSE =',best_mse)
  J_values = list(results.keys())
  distances = list(results.values())
  min_distance, corresponding_J = min(zip(distances, J_values))  # Finds the minimum distance and its corresponding J value
  print(f"For sample size {n}, the lowest distance is: {min_distance}, corresponding to J = {corresponding_J}")
  ax.loglog(J_values, distances, marker='o', label=f'Sample Size {n}')
ax.set_xlabel('J (number of partitions)')
ax.set_ylabel('Best Distance')
ax.set_title('Partition estimator on less-smooth function (B lipschitz =100)')
ax.legend()
plt.tight_layout()
plt.show()

"""# **2- K Nearest Neighbors**

"""

def knn_estimator(x_train, y_train, x_validation, k):
    knn_model = KNeighborsRegressor(n_neighbors=k)
    knn_model.fit(x_train.reshape(-1, 1), y_train)
    y_pred = knn_model.predict(x_validation.reshape(-1, 1))
    return y_pred

def distance_function_knn(x, y, k, f_star, n_mc):
    x_mc = np.random.rand(n_mc, 1)
    y_pred = knn_estimator(x, y,x_mc, k)
    distance = np.mean((np.abs(y_pred - f_star(x_mc)))**2)
    return distance
#Monte Carlo method
def MSE(fhat, x_valid, y_valid):
    mse = np.mean((np.abs(fhat - y_valid))**2)
    return mse

result_table_knn = []
fig, ax = plt.subplots(2, 1, figsize=(10,8))
for n in (sample_sizes):
    results = {}
    true_theoretical_k = np.ceil((((sigma**2) * n)/(8))**(1/2))
    k_list = [ round(true_theoretical_k * scale) for scale in scale_list if round(true_theoretical_k * scale) < n*(1-validation_size)  and round(true_theoretical_k * scale) != 0]
    k_list = np.unique(k_list)
    best_mse = float('inf')
    best_k = float('inf')
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
    best_mses = []
    best_hyperparams = []
    for k in k_list:
        time_start = time.time()
        f_kvalidation = knn_estimator(x, y, x_validation, k)
        mse = MSE(f_kvalidation, x_validation, y_validation)
        time_end = time.time()
        T = time_end-time_start
        distance = distance_function_knn(x, y, k, f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_k = k
        c_t = T / (n*np.log(n))
        results[k] = distance
        result_table_knn.append({
            "Sample Size": n,
            "Partition Size": k,
            "Best J": best_k,
            "T": T,
            "c_t": c_t,
            "Distance": distance,
            "MSE": mse,
        })
        best_mses.append(mse)
        best_hyperparams.append(k)
    k_values = list(results.keys())
    distances = list(results.values())
    min_k = min(results, key=results.get)
    min_distance = results[min_k]
    #print(f"For sample size {n}, k_t : {true_theoretical_k}, k_p= {min_k}, min distance = {min_distance} ")
    ax[0].loglog(k_values, distances, marker='o', label=f'Sample Size {n}')
    ax[1].loglog(best_hyperparams, best_mses, marker='o', label=f'Sample Size {n}')

ax[0].set_xlabel('number of neighbors')
ax[0].set_ylabel('Best Distance')
ax[0].set_title('Best Distance by k ')
ax[0].legend()
ax[1].set_xlabel('number of neighbors ')
ax[1].set_ylabel('MSE on Validation')
ax[1].set_title('MSE on Validation by k')
ax[1].legend()
plt.tight_layout()
plt.show()
result_table_knn = pd.DataFrame(result_table_knn).sort_values('Sample Size')

result_table_knn['c_t']
k_factor = np.mean(c_t)
Time_budget=1
print(k_factor)
New_scale_listk = [Time_budget /(k_factor  * (n*np.log(n))) for  n in sample_sizes]
print(New_scale_listk)
print(len(New_scale_listk))

fig, ax = plt.subplots(figsize=(10,8))
for idx, n in enumerate(sample_sizes):
    scale_ = int(New_scale_listk[idx])
    results = {}
    B=0.05
    not_so_theoretical_k = int(np.ceil(((sigma**2)*n/(8*(B**2)))**(1/2)))
    #print('not_so_theoretical_k',not_so_theoretical_k)
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size= validation_size)
    knn_estimator(x, y, x_validation, not_so_theoretical_k)
    k_list_log = np.linspace(0,2* np.log(not_so_theoretical_k),num=scale_)
    k_list = np.unique(np.round(np.exp(k_list_log)))
    if len(k_list) < scale_:
      k_list = np.concatenate((k_list,np.arange(2,min(scale_-len(k_list),n))))
      k_list = np.unique(k_list)
      #print('J_list_merged',J_list)
    if len(k_list) <=44:
      k_list=k_list
    if not_so_theoretical_k in k_list:
      print('k_(not theory) exists')
    best_mse = float('inf')
    best_mses = []
    best_hyperparams = []
    for k in k_list:
       if k <= len(x_validation):
        time_start = time.time()
        fk_validation = knn_estimator(x, y, x_validation, int(k))
        mse = MSE(fk_validation, x_validation, y_validation)
        time_end = time.time()
        T = time_end - time_start
        distance = distance_function_knn(x, y, int(k), f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_k = k
        results[k] = distance
        # print(len(list(results.keys())))
        best_mses.append(mse)
        best_hyperparams.append(k)
    k_values = list(results.keys())
    distances = list(results.values())
    min_distance, corresponding_k = min(zip(distances, k_values))
    print('for N=',n,' best no of neighbors',best_k,'with best MSE=',best_mse,'with Best distance',{min_distance})
    ax.loglog(k_values, distances, marker='o', label=f'Sample Size {n}')
ax.set_xlabel('k')
ax.set_ylabel('Best Distance')
ax.set_title('K Nearest Neighbors on smooth function (B lipschitz =0.05)')
ax.legend()
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(10,8))
for idx, n in enumerate(sample_sizes):
    scale_ = int(New_scale_listk[idx])
    results = {}
    B=100
    not_so_theoretical_k = int(np.ceil(((sigma**2)*n/(8*(B**2)))**(1/2)))
    #print('not_so_theoretical_k',not_so_theoretical_k)
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size= validation_size)
    knn_estimator(x, y, x_validation, not_so_theoretical_k)
    k_list_log = np.linspace(0,2* np.log(not_so_theoretical_k),num=scale_)
    k_list = np.unique(np.round(np.exp(k_list_log)))
    if len(k_list) < scale_:
      k_list = np.concatenate((k_list,np.arange(2,min(scale_-len(k_list),n))))
      k_list = np.unique(k_list)
      #print('J_list_merged',J_list)
    if len(k_list)==1:
      k_list=[not_so_theoretical_J]
    if not_so_theoretical_k in k_list:
      print('k_(not theory) exists')
    best_mse = float('inf')
    best_mses = []
    best_hyperparams = []
    for k in k_list:
       if k <= len(x_validation):
        time_start = time.time()
        fk_validation = knn_estimator(x, y, x_validation, int(k))
        mse = MSE(fk_validation, x_validation, y_validation)
        time_end = time.time()
        T = time_end - time_start
        distance = distance_function_knn(x, y, int(k), f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_k = k
        results[k] = distance
        # print(len(list(results.keys())))
        best_mses.append(mse)
        best_hyperparams.append(k)
    k_values = list(results.keys())
    distances = list(results.values())
    min_distance, corresponding_k = min(zip(distances, k_values))
    print('for N=',n,' best no of neighbors',best_k,'with best MSE=',best_mse,'with Best distance',min_distance)
    ax.loglog(k_values, distances, marker='o', label=f'Sample Size {n}')
ax.set_xlabel('k')
ax.set_ylabel('Best Distance')
ax.set_title('K Nearest Neighbors on less smooth function (B lipschitz =100)')
ax.legend()
plt.tight_layout()
plt.show()

"""# **3- Kernel regression**
## I started with this model, but due to time limitations, I didn't have the time to achieve the expected results from it (I will continue to work on this later).

"""

def kernel_estimator(Xtrain, ytrain, Xtest, h):
  ftest = np.zeros_like(Xtest)
  for itest in range(len(Xtest)):
      x = Xtest[itest]
      temp = np.exp(-((x - Xtrain)**2) / (2 * (h**2)))
      ftest[itest] = np.dot(temp.T, ytrain) / np.sum(temp)
  return ftest
def MSE(fhat, x_valid, y_valid):
    mse = np.mean((np.abs(fhat - y_valid))**2)
    return mse

def distance_function_kernel(x, y, h, f_star, n_mc):
    x_mc = np.random.rand(n_mc,1)
    f_hat = kernel_estimator(x, y, x_mc, h)
    distance = np.mean((np.abs(f_hat - f_star(x_mc)))**2)
    return distance

result_table_h = []
fig, ax = plt.subplots(2, 1, figsize=(10,8))
for n in sample_sizes_ct:
    results = {}
    B=1
    m=np.sqrt(2*np.pi)
    true_theoretical_h = (3/(m*n*B))**(1/3)
    h_list = [ (true_theoretical_h * scale) for scale in scale_list if round(true_theoretical_h * scale) < n*(1-validation_size)]
    h_list = np.unique(h_list)
    h_list=h_list[h_list != 0]
    print('true_theoretical_h',true_theoretical_h,'h_list',h_list)
    best_mse = float('inf')
    best_h = float('inf')
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma**2, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
    best_mses = []
    best_hyperparams = []
    for h in h_list:
        time_start = time.time()
        fh_validation = kernel_estimator(x, y, x_validation, h)
        mse = MSE(fh_validation, x_validation, y_validation)
        time_end = time.time()
        T = time_end-time_start
        distance = distance_function_kernel(x, y, h, f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_h = h
        c_t = T / (n**2)
        results[h] = distance
        result_table_h.append({
            "Sample Size": n,
            "Partition Size": h,
            "Best h": best_h,
            "T": T,
            "c_t": c_t,
            "Distance": distance,
            "MSE": mse,
        })
        best_mses.append(mse)
        best_hyperparams.append(h)
    h_values = list(results.keys())
    distances = list(results.values())
    min_h = min(results, key=results.get)
    min_distance = results[min_h]
    #print(f"For sample size {n}, h_t : {true_theoretical_h}, h_p= {min_h}, min distance = {min_distance} ")
    ax[0].loglog(h_values, distances, marker='o', label=f'Sample Size {n}')
    ax[1].loglog(best_hyperparams, best_mses, marker='o', label=f'Sample Size {n}')

ax[0].set_xlabel('Bandwidth')
ax[0].set_ylabel('Best Distance')
ax[0].set_title('Best Distance by h ')
ax[0].legend()
ax[1].set_xlabel('number of partitions ')
ax[1].set_ylabel('MSE on Validation')
ax[1].set_title('MSE on Validation by J')
ax[1].legend()
plt.tight_layout()
plt.show()
result_table_h = pd.DataFrame(result_table_h).sort_values('Sample Size')

result_table_h['c_t']
h_factor = np.mean(c_t)
print(h_factor)
Time_budget = 1
New_scale_listh = [Time_budget /(h_factor  * (n**2)) for  n in sample_sizes]
print(New_scale_listh)
print(len(New_scale_listh))

fig, ax = plt.subplots(figsize=(7,7))
for idx, n in enumerate(sample_sizes[3:]):
    print('n',n)
    x = np.random.rand(n,1)
    eps = np.random.normal(0,sigma**2, n)
    f_star = lambda x_ : np.sin(2*np.pi*x_.ravel())
    y = f_star(x) + eps
    x, x_validation, y, y_validation = train_test_split(x, y, test_size=validation_size)
    scale_ = int(New_scale_listh[3+idx])
    results = {}
    B=0.2
    m=np.sqrt(2*np.pi)
    not_so_theoretical_h =  (3/(m*n*B))**(1/3)
    print('not_so_theoretical_h',not_so_theoretical_h)
    kernel_estimator(x, y, x_validation, not_so_theoretical_h)
    h_list_log = np.linspace(-3+np.log(not_so_theoretical_h) ,3+np.log(not_so_theoretical_h),num=scale_)
    h_list = np.exp(h_list_log)
    print('length of h lsit',len(h_list))
    best_mse = float('inf')
    best_mses = []
    best_hyperparams = []
    for h in h_list:
        fh_validation = kernel_estimator(x, y, x_validation, h)
        mse = MSE(fh_validation, x_validation, y_validation)
        T = time_end - time_start
        distance = distance_function_kernel(x, y, h, f_star, 1000)
        if mse < best_mse:
            best_mse = mse
            best_p = h
        results[h] = distance
        best_mses.append(mse)
        best_hyperparams.append(h)
    h_values = list(results.keys())
    distances = list(results.values())
    ax.loglog(h_values, distances, marker='o', label=f'Sample Size {n}')
ax.set_xlabel('h ')
ax.set_ylabel('Best Distance')
ax.set_title('Best Distance by h')
ax.legend()
plt.tight_layout()
plt.show()